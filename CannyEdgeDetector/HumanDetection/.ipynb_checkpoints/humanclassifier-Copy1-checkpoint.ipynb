{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import cv2\n",
    "import math as m\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImagePathsWithOutput(path_dir_output):\n",
    "    image_paths = []\n",
    "    output = []\n",
    "    for folder in path_dir_output.keys():\n",
    "        for dirt, subdirt, fileList in os.walk(folder):\n",
    "            for file in fileList:\n",
    "                image_paths.append(folder + file)\n",
    "                output.append(path_dir_output[folder])\n",
    "    return image_paths, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main\n",
    "root = 'ImageData/' \n",
    "train_pos_dir = root + 'Training_images_Pos/'\n",
    "train_neg_dir = root + 'Training_images_Neg/' \n",
    "test_pos_dir = root + 'Test_images_Pos/' \n",
    "test_neg_dir = root + 'Test_images_Neg/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir_with_output = {\n",
    "    train_pos_dir:1,\n",
    "    train_neg_dir:0\n",
    "}\n",
    "test_data_dir_with_output = {\n",
    "    test_neg_dir:0,\n",
    "    test_pos_dir:1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image_paths, training_output = getImagePathsWithOutput(train_data_dir_with_output)\n",
    "testing_image_paths, testing_output = getImagePathsWithOutput(test_data_dir_with_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ImageData/Training_images_Pos/crop001047b.bmp',\n",
       " 'ImageData/Training_images_Pos/crop001045b.bmp',\n",
       " 'ImageData/Training_images_Pos/crop_000010b.bmp',\n",
       " 'ImageData/Training_images_Pos/crop001275b.bmp',\n",
       " 'ImageData/Training_images_Pos/crop001028a.bmp',\n",
       " 'ImageData/Training_images_Pos/crop001008b.bmp',\n",
       " 'ImageData/Training_images_Pos/crop001030c.bmp',\n",
       " 'ImageData/Training_images_Pos/person_and_bike_026a.bmp',\n",
       " 'ImageData/Training_images_Pos/crop001063b.bmp',\n",
       " 'ImageData/Training_images_Pos/crop001672b.bmp',\n",
       " 'ImageData/Training_images_Neg/00000093a_cut.bmp',\n",
       " 'ImageData/Training_images_Neg/00000062a_cut.bmp',\n",
       " 'ImageData/Training_images_Neg/no_person__no_bike_247_cut.bmp',\n",
       " 'ImageData/Training_images_Neg/no_person__no_bike_213_cut.bmp',\n",
       " 'ImageData/Training_images_Neg/01-03e_cut.bmp',\n",
       " 'ImageData/Training_images_Neg/00000053a_cut.bmp',\n",
       " 'ImageData/Training_images_Neg/no_person__no_bike_259_cut.bmp',\n",
       " 'ImageData/Training_images_Neg/00000091a_cut.bmp',\n",
       " 'ImageData/Training_images_Neg/00000057a_cut.bmp',\n",
       " 'ImageData/Training_images_Neg/no_person__no_bike_219_cut.bmp']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOGFeature:\n",
    "    \n",
    "    def rgb2gray(self, img):\n",
    "        '''\n",
    "        function to convert image from color image to grayscale image\n",
    "        img = the image matrix\n",
    "        '''\n",
    "        gray = np.zeros((img.shape[0],img.shape[1]))\n",
    "        for i in range(0,img.shape[0]):\n",
    "            for j in range(0,img.shape[1]):\n",
    "                bgr = img[i,j]\n",
    "                b = bgr[0]\n",
    "                g = bgr[1]\n",
    "                r = bgr[2]\n",
    "                gray[i,j] = round(0.299*r+0.587*g+0.114*b)\n",
    "        return gray\n",
    "    \n",
    "    def apply_sobel(self, img):\n",
    "        '''\n",
    "        Function to compute Normalized Gradient Magnitude and Gradient Angle\n",
    "        '''\n",
    "        #Sobel Filter Mask to calculate Horizontal(x) gradient\n",
    "        sobel_x = (1/4)*np.array([(-1,0,1),\n",
    "                    (-2,0,2),\n",
    "                    (-1,0,1)])\n",
    "        #Sobel Filter Mask to calculate Vertical(y) gradient\n",
    "        sobel_y = (1/4)*np.array([(1,2,1),\n",
    "                    (0,0,0),\n",
    "                    (-1,-2,-1)])\n",
    "\n",
    "        #initialize matrices to store the value of horizontal and vertical gradient,\n",
    "        #normalized horizontal and vertical gradient, normalized gradient magnitude\n",
    "        #and gradient angle\n",
    "        gradient_magnitude = np.zeros(shape=img.shape)\n",
    "        gradient_angle = np.zeros(shape=img.shape)\n",
    "        #find the gradient values by perfoeming convolution\n",
    "        for row in range(0,img.shape[0]-2):\n",
    "            for col in range(0,img[row].size-2):\n",
    "                #calculate Value at current pixel (row,col) \n",
    "                #after applying sobel operator\n",
    "                gx, gy = 0, 0\n",
    "                for i in range (0,3):\n",
    "                    for j in range (0,3):\n",
    "                        gx = gx + img[row+i][col+j] * sobel_x[i][j]\n",
    "                        gy = gy + img[row+i][col+j] * sobel_y[i][j]\n",
    "                #normalize gradient magnitude by dividing by sqrt(2)\n",
    "                gradient_magnitude[row+1][col+1]=((gx**2+gy**2)**(0.5))/(1.4142)\n",
    "                #calculate gradient angle based on sobel horizontal gradient and vertical gradient\n",
    "                angle = 0\n",
    "                if(gx == 0):\n",
    "                    if( gy > 0):\n",
    "                        angle = 90\n",
    "                    else:\n",
    "                        angle = -90\n",
    "                else:\n",
    "                    angle = math.degrees(math.atan(gy/gx))\n",
    "                if (angle < 0):\n",
    "                    angle = angle + 180\n",
    "                gradient_angle[row+1,col+1]  = angle\n",
    "        return [gradient_magnitude, gradient_angle]\n",
    "    \n",
    "    def get_cell_histogram(self, gradient_magnitude, gradient_angle):\n",
    "        '''\n",
    "        This function is to get histogram for each 8x8 cell\n",
    "        gradient_magnitude = gradient magnitude for each pixel\n",
    "        gradient_angle = gradient angle for each pixel\n",
    "        '''\n",
    "        cell_shape = gradient_magnitude.shape\n",
    "        #initialize the number of cell rows and cell columns\n",
    "        cell_rows = round(cell_shape[0]/8)\n",
    "        cell_cols = round(cell_shape[1]/8)\n",
    "        histogram_cell = np.zeros((cell_rows,cell_cols,9))\n",
    "        for r in range (0,cell_rows-1):\n",
    "            for c in range (0,cell_cols-1):\n",
    "                for row in range (r*8,r*8+8):\n",
    "                    for col in range (c*8,c*8+8):\n",
    "                        angle = gradient_angle[row][col]\n",
    "                        grad_mag = gradient_magnitude[row][col]\n",
    "                        if angle%20 == 0:\n",
    "                            if angle == 180:\n",
    "                                histogram_cell[r][c][0] += grad_mag\n",
    "                                continue\n",
    "                            bin_no = int(angle/20)\n",
    "                            histogram_cell[r][c][bin_no] += grad_mag\n",
    "                            continue\n",
    "                        bin_no_l = int(angle/20)\n",
    "                        #calculate the vote for left and right bins.\n",
    "                        if bin_no_l < 8:\n",
    "                            bin_no_r = bin_no_l + 1\n",
    "                            histogram_cell[r][c][bin_no_r] += grad_mag*((angle - (bin_no_l * 20))/20)\n",
    "                            histogram_cell[r][c][bin_no_l] += grad_mag*(((bin_no_r * 20) - angle)/20)\n",
    "                        else:\n",
    "                            bin_no_r = 0\n",
    "                            histogram_cell[r][c][bin_no_r] += grad_mag*((angle - 160)/20)\n",
    "                            histogram_cell[r][c][bin_no_l] += grad_mag*((180 - angle)/20)\n",
    "        squared_histogram_cell = np.square(histogram_cell)\n",
    "        return [histogram_cell, squared_histogram_cell] \n",
    " \n",
    "    \n",
    "    def get_hog_descriptor(self, histogram_cell, histogram_cell_squared):\n",
    "        cell_histogram_shape = histogram_cell.shape\n",
    "        descriptor = np.array([])\n",
    "        for row in range(0,cell_histogram_shape[0]-1):\n",
    "            for col in range(0,cell_histogram_shape[1]-1):\n",
    "                block = np.array([])\n",
    "                block_squared = np.array([])\n",
    "                block = np.append(block,histogram_cell[row,col])\n",
    "                block = np.append(block,histogram_cell[row,col+1])\n",
    "                block = np.append(block,histogram_cell[row+1,col])\n",
    "                block = np.append(block,histogram_cell[row+1,col+1])\n",
    "                block_squared = np.append(block_squared,histogram_cell_squared[row,col])\n",
    "                block_squared = np.append(block_squared,histogram_cell_squared[row,col+1])\n",
    "                block_squared = np.append(block_squared,histogram_cell_squared[row+1,col])\n",
    "                block_squared = np.append(block_squared,histogram_cell_squared[row+1,col+1])\n",
    "                block_squared = np.sum(block_squared)\n",
    "                if(block_squared>0):\n",
    "                    #normalize the block descriptor\n",
    "                    normalized = np.sqrt(block_squared)\n",
    "                    block = (1/normalized)*block\n",
    "                descriptor = np.append(descriptor, block)\n",
    "        return descriptor\n",
    "\n",
    "    \n",
    "    def get_LBP_descriptor(self, magnitude):\n",
    "        '''\n",
    "        This function is to get histogram for each 8x8 cell\n",
    "        gradient_magnitude = gradient magnitude for each pixel\n",
    "        gradient_angle = gradient angle for each pixel\n",
    "        '''\n",
    "        pattern = [(-1, -1),(-1, 0),(-1, 1),(0, -1),(0, 1),(1, -1),(1, 0),(1, 1)]\n",
    "        uniform_patterns = np.array([0, 1, 2, 3, 4, 6, 7, 8, 12, 14, 15, 16, 24, 28, 30, 31, 32, 48, 56,\n",
    "        60, 62, 63, 64, 96, 112, 120, 124, 126, 127, 128, 129, 131, 135, 143, 159, 191, 192, 193, 195, 199,\n",
    "        207, 223, 224, 225, 227, 231, 239, 240, 241, 243, 247, 248, 249, 251, 252, 253, 254, 255])\n",
    "        non_uniform_patterns = np.array(list(set(np.arange(256))-set(uniform_patterns)))\n",
    "        cell_shape = magnitude.shape\n",
    "        #initialize the number of cell rows and cell columns\n",
    "        cell_rows = round(cell_shape[0]/16)\n",
    "        cell_cols = round(cell_shape[1]/16)\n",
    "        histogram_block = np.zeros((cell_rows,cell_cols,59))\n",
    "        lbp_mask = np.zeros(magnitude.shape)\n",
    "        lbp_descriptor = np.array([])\n",
    "        for r in range (0,cell_rows):\n",
    "            for c in range (0,cell_cols):\n",
    "                histogram_block = np.zeros(256)\n",
    "                for row in range (r*16,r*16+16):\n",
    "                    for col in range (c*16,c*16+16):\n",
    "                        if row == 0 or col == 0 or row == magnitude.shape[0]-1 or col == magnitude.shape[1]-1:\n",
    "                            lbp_mask[row][col] = 5\n",
    "                            continue\n",
    "                        bin_val = ''\n",
    "                        mag = magnitude[row][col]\n",
    "                        for i,j in pattern:\n",
    "                            if magnitude[row+i][col+j] >= mag:\n",
    "                                bin_val += '1'\n",
    "                            else:\n",
    "                                bin_val += '0'\n",
    "                        histogram_block[int(bin_val,2)] += 1\n",
    "                normalized_histogram = histogram_block/256\n",
    "                bin_59 = sum(normalized_histogram[non_uniform_patterns])\n",
    "                bin_1_58 = normalized_histogram[uniform_patterns]\n",
    "                bin_1_59 = np.append(bin_1_58, bin_59)\n",
    "                lbp_descriptor = np.append(lbp_descriptor,bin_1_59)\n",
    "        return lbp_descriptor\n",
    "    \n",
    "    \n",
    "    def saveImageAndHog(self, mag, hog, lbp, path):\n",
    "        \"\"\"\n",
    "        function to save the image and hog descriptor\n",
    "        \"\"\"\n",
    "        pathSplit = path.split('/')\n",
    "        currImage = pathSplit[-1]\n",
    "        imageName, imageExt = currImage.split('.')\n",
    "        updatedImage = '.'.join([imageName+'_mag',imageExt])\n",
    "        imageFolder = '/'.join(pathSplit[:-1])\n",
    "        imageFolder += \"_res\"\n",
    "        if not os.path.exists(imageFolder):\n",
    "            os.makedirs(imageFolder)\n",
    "        finalPath = '/'.join([imageFolder,updatedImage])\n",
    "        cv2.imwrite(finalPath, mag)\n",
    "        filename = imageFolder+\"/\"+imageName+\"_hog.txt\"\n",
    "        hog_file = open(filename,'a+')\n",
    "        for i in hog:\n",
    "            hog_file.write(str(i[0])+'\\n')\n",
    "        hog_file.close()\n",
    "        filename = imageFolder+\"/\"+imageName+\"_lbp.txt\"\n",
    "        lbp_file = open(filename,'a+')\n",
    "        for i in lbp:\n",
    "            lbp_file.write(str(i[0])+'\\n')\n",
    "        lbp_file.close()\n",
    "        \n",
    "    def HogLbp(self, image_list):\n",
    "        hog_features = []\n",
    "        lbp_features = []\n",
    "        for image in image_list:\n",
    "            color_img = cv2.imread(image,cv2.IMREAD_COLOR)\n",
    "            gray_img = self.rgb2gray(color_img)\n",
    "            gradient_magnitude, gradient_angle = self.apply_sobel(gray_img)\n",
    "            histogram = self.get_cell_histogram(gradient_magnitude, gradient_angle)\n",
    "            histogram_cell = histogram[0]\n",
    "            squared_histogram_cell = histogram[1]\n",
    "            HOG_descriptor = self.get_hog_descriptor(histogram_cell, squared_histogram_cell)\n",
    "            HOG_descriptor = HOG_descriptor.reshape(-1,1)\n",
    "            LBP_descriptor = self.get_LBP_descriptor(gradient_magnitude)\n",
    "            LBP_descriptor = LBP_descriptor.reshape(-1,1)\n",
    "            self.saveImageAndHog(gradient_magnitude, HOG_descriptor, LBP_descriptor, image)\n",
    "            hog_features.append(HOG_descriptor)\n",
    "            lbp_features.append(LBP_descriptor)\n",
    "        return hog_features,lbp_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "\n",
    "    \"\"\"\n",
    "    initialize the neural network\n",
    "    graph = the shape of the neural network\n",
    "    ep = the number of epocs\n",
    "    lr = learning rate\n",
    "    patience = number of epocs to continue after error changes very little \n",
    "    \"\"\"\n",
    "    def __init__(self, graph = (7524, 1000, 1), ep = 100, lr = 0.1, patience = 3):\n",
    "        \n",
    "        self.graph = graph\n",
    "        #initialize the weights randomly\n",
    "        np.random.seed(10)\n",
    "        self.w1 = np.random.random((graph[1], graph[0]))\n",
    "        np.random.seed(20)\n",
    "        self.w2 = np.random.random((graph[2], graph[1]))\n",
    "\n",
    "        #initialize bias for hidden layer and output to zero\n",
    "        self.b1 = np.array([-1]*graph[1]).reshape(-1,1)\n",
    "        self.b2 = np.array([-1]*graph[1]).reshape(-1,1)\n",
    "\n",
    "        #intermediate values to remember\n",
    "        self.z1 = None\n",
    "        self.a1 = None\n",
    "        self.z2 = None\n",
    "        self.output = None\n",
    "\n",
    "        self.ep = ep\n",
    "        self.lr = lr\n",
    "        self.patience = patience\n",
    "    \n",
    "    \"\"\"\n",
    "    feed forward, calculate the output of the neural net for given input\n",
    "    \"\"\"\n",
    "    def ff(self, td):\n",
    "        #calculate the input for the hidden layer [input*w1 + b1] \n",
    "        self.z1 = self.w1.dot(td) + self.b1\n",
    "        #activation for hidden layer using ReLU\n",
    "        self.a1 = self.ReLU(self.z1)\n",
    "        #calculate the input for output layer [activation1*w2 + b2]\n",
    "        self.z2 = self.w2.dot(self.a1) + self.b2\n",
    "        #activation for output nodes using sigmoid\n",
    "        self.output = self.sigmoid(self.z2)\n",
    "\n",
    "    \"\"\"\n",
    "    function to calculate squared error from the output and expected value\n",
    "    \"\"\"\n",
    "    def err(self, expected_output):\n",
    "        return np.square(self.output - expected_output).sum()\n",
    "    \n",
    "    \"\"\"\n",
    "    function to perform back propogation\n",
    "    \"\"\"\n",
    "    def bp(self, td, expedted_output):\n",
    "        diff =  self.output - expedted_output\n",
    "        t2 = 2 * diff * self.dsigmoid(self.output)\n",
    "        self.d_w2 = np.dot(t2 ,self.a1.T)\n",
    "\n",
    "        t1 = np.dot(self.w2.T,t2) * self.dReLU(self.a1)\n",
    "        self.d_w1 = np.dot(t1,td.T)\n",
    "\n",
    "        self.d_b2 = np.sum(t2, axis = 1, keepdims = True)\n",
    "        self.d_b1 = np.sum(t1, axis = 1, keepdims = True)\n",
    "    \n",
    "    \"\"\"\n",
    "    update the weights and bias of the neural net\n",
    "    \"\"\"\n",
    "    def update(self):\n",
    "        self.w1 = self.w1 - (self.d_w1 * self.lr)\n",
    "        self.b1 = self.b1 - (self.d_b1 * self.lr)\n",
    "\n",
    "        self.w2 = self.w2 - (self.d_w2 * self.lr)\n",
    "        self.b2 = self.b2 - (self.d_b2 * self.lr)\n",
    "    \n",
    "    \"\"\"\n",
    "    function to train the neural net\n",
    "    training_data = input training data\n",
    "    label = expected output\n",
    "    \"\"\"\n",
    "    def train(self, training_data, label):\n",
    "        dataLen = len(training_data)\n",
    "        sn = np.arange(dataLen)\n",
    "        random.shuffle(sn)\n",
    "        prev_err = sys.maxsize\n",
    "        for epoch in range(self.ep):\n",
    "            ep_err = 0.0 #initialize error for current epoch to zero\n",
    "            #train the network for each image and update the weights accordingly\n",
    "            for count in sn:\n",
    "                train_data = training_data[count]\n",
    "                self.ff(train_data)\n",
    "                error = self.err(label[count])\n",
    "                ep_err += error\n",
    "                self.bp(train_data, label[count])\n",
    "                self.update()\n",
    "            ep_err = ep_err/dataLen\n",
    "            print(\"Epoch Count: \" + str(epoch), \"Average Error: \", ep_err)\n",
    "            if(ep_err < prev_err):\n",
    "                print(\"error decreased by \", prev_err-ep_err)\n",
    "            else:\n",
    "                if(ep_err > prev_err):\n",
    "                    print(\"error increased by \", ep_err-prev_err)\n",
    "                else:\n",
    "                    print(\"error stayed same\")\n",
    "            #check for the change in error if very less we can stop training\n",
    "            if(prev_err - ep_err < 0.000000001):\n",
    "                self.patience -= 1\n",
    "                if(self.patience == 0):\n",
    "                    print(\"training complete....\")\n",
    "                    break\n",
    "            #save the error for comparison with next epoch\n",
    "            prev_err = ep_err\n",
    "        #save the weights and bias of the network to a file\n",
    "        self.saveState()\n",
    "\n",
    "    \"\"\"\n",
    "    funciton to test the network with the testing data\n",
    "    \"\"\"\n",
    "    def test(self, testImages, testing_data, label):\n",
    "        misclassify = 0\n",
    "        positiveList = []\n",
    "        negativeList = []\n",
    "        for count, test_data in enumerate(testing_data):\n",
    "            self.ff(test_data)\n",
    "            cPrediction = np.round(self.output[0])\n",
    "            print(\"image: \", testImages[count])\n",
    "            print(\"Predicted Probability: \" + str(self.output.sum()), \"Actual Probability Value: \" + str(label[count]))\n",
    "            if cPrediction:\n",
    "                positiveList.append([testImages[count], str(self.output.sum())])\n",
    "            else:\n",
    "                negativeList.append([testImages[count], str(self.output.sum())])\n",
    "            if(cPrediction - label[count] != 0):\n",
    "                print(\"misclassified!!!!!!!!!!!!!!!!\")\n",
    "                misclassify += 1\n",
    "        print(str(float(len(label)-misclassify) / float(len(label)) * 100) + \" % Prediction Accuracy\")\n",
    "\n",
    "    \"\"\"\n",
    "    function to save the weights and bias of the network\n",
    "    \"\"\"\n",
    "    def saveState(self):\n",
    "        np.savetxt(\"weights1.csv\", self.w1, delimiter=',')\n",
    "        np.savetxt(\"weights2.csv\", self.w2, delimiter=',')\n",
    "        np.savetxt(\"bias1.csv\", self.b1, delimiter=',')\n",
    "        np.savetxt(\"bias2.csv\", self.b2, delimiter=',')\n",
    "    \n",
    "    #sigmoid activation function\n",
    "    def sigmoid(self, x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    def dsigmoid(self, x):\n",
    "        return x*(1-x)\n",
    "\n",
    "    #ReLU activation function\n",
    "    def ReLU(self,x):\n",
    "        return x*(x>0)\n",
    "    def dReLU(self, x):\n",
    "        return 1*(x>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ReLU(x):\n",
    "    \"\"\"Function to calculate RELU\n",
    "    \"\"\"\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            if x[i,j]<0:\n",
    "                x[i,j]=0\n",
    "    return x\n",
    "\n",
    "def Sigmoid(x):\n",
    "    \"\"\"\n",
    "    Function to calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def neural_net(network,out,hidden):\n",
    "    \"\"\"\n",
    "    Function to implament neural network, to train it.\n",
    "    \"\"\"\n",
    "    aplha = 0.1                                               \n",
    "    # Initializing the learning rate\n",
    "    total_hidden = network.shape[1]                                         \n",
    "    total_output = network.shape[2]\n",
    "    # Initializing and Factoring the weight for hidden layer\n",
    "    w1 = np.random.randn(total_hidden,hidden)                          \n",
    "    w1 = np.multiply(w1,math.sqrt(2/int(total_hidden+hidden)))\n",
    "    # Initializing and Factoring the weight for output layer         \n",
    "    w2 = np.random.randn(hidden,total_output)\n",
    "    w2 = np.multiply(w2,math.sqrt(2/int(hidden+total_output)))\n",
    "    # Bias for hidden and output layer\n",
    "    b1 = np.random.randn(hidden)                         \n",
    "    b1 = np.multiply(b1,math.sqrt(2/int(hidden)))\n",
    "    b2 = np.random.randn(total_output)\n",
    "    b2 = np.multiply(b2,math.sqrt(2/int(total_output)))\n",
    "    epoch = 0\n",
    "    err_sq = 0\n",
    "    prev_err = sys.maxsize\n",
    "    while True:                                         \n",
    "        # FeedForward and Backpropagation for each epoch of all vectors\n",
    "        for i in range(network.shape[0]):\n",
    "            x = network[i,:].reshape([1,-1])                     \n",
    "            # Computing values for hidden layer and output layer in feedforward\n",
    "            layer1_output = ReLU((x.dot(w1)+b1))\n",
    "            layer2_output = Sigmoid((layer1_output.dot(w2)+b2))\n",
    "            err = out[i]-layer2_output           \n",
    "            err_sq += 0.5*err*err\n",
    "            #Doing BackPropagation\n",
    "            del_output = (-1*err)*(1-layer2_output)*layer2_output                         \n",
    "            del_layer2 = layer1_output.T.dot(del_output)\n",
    "            del_bias_layer2 = np.sum(del_output,axis = 0)\n",
    "            layer1_output_like = np.zeros_like(layer1_output)\n",
    "            for k in range(hidden):\n",
    "                if(layer1_output[0][k]>0):\n",
    "                    layer1_output_like[0][k] = 1\n",
    "                else:\n",
    "                    layer1_output_like[0][k] = 0                       \n",
    "            del_hidden = del_output.dot(w2.T)*layer1_output_like\n",
    "            del_layer1 = x.T.dot(del_hidden)\n",
    "            del_bias_layer1 = np.sum(del_hidden,axis=0)\n",
    "\n",
    "            w2 -= aplha*del_layer2\n",
    "            b2 -= aplha*del_bias_layer2\n",
    "            w1 -= aplha*del_layer1\n",
    "            b1 -= aplha*del_bias_layer1\n",
    "        ep_err = np.mean(err_sq)/network.shape[0]\n",
    "        print(\"Epoch Count: \" + str(epoch), \"Average Error: \", ep_err)\n",
    "        if(ep_err < prev_err):\n",
    "            print(\"error decreased by \", prev_err-ep_err)\n",
    "        else:\n",
    "            if(ep_err > prev_err):\n",
    "                print(\"error increased by \", ep_err-prev_err)\n",
    "            else:\n",
    "                print(\"error stayed same\")\n",
    "        #check for the change in error if very less we can stop training\n",
    "        if(abs(prev_err - ep_err) < 0.0001):\n",
    "            print(\"training complete....\")\n",
    "            break\n",
    "        prev_err = ep_err\n",
    "        epoch += 1\n",
    "\n",
    "    return w1,b1,w2,b2\n",
    "\n",
    "\n",
    "# Function to predict values for my neural network\n",
    "\n",
    "def predict(w,wb,v,vb,output_descriptor):\n",
    "    Number_of_test_image,number_of_attribute=Output_descriptor.shape\n",
    "    predict=[]\n",
    "    for k in range(Number_of_test_image):\n",
    "            x=Output_descriptor[k,:].reshape([1,-1])\n",
    "            z=ReLU((x.dot(w)+wb))\n",
    "            y=sigmoid(z.dot(v)+vb)\n",
    "            predict.append(y)\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07669464228702358 -0.0798755930568931\n",
      "Epoch Count: 0 Average Error:  0.09840887899004942\n",
      "error decreased by  9.223372036854776e+18\n",
      "Epoch Count: 1 Average Error:  0.14956368459789948\n",
      "error increased by  0.05115480560785006\n",
      "Epoch Count: 2 Average Error:  0.17301756559739898\n",
      "error increased by  0.023453880999499493\n",
      "Epoch Count: 3 Average Error:  0.18797516197725625\n",
      "error increased by  0.014957596379857274\n",
      "Epoch Count: 4 Average Error:  0.19918678958007602\n",
      "error increased by  0.011211627602819768\n",
      "Epoch Count: 5 Average Error:  0.2077687133358186\n",
      "error increased by  0.008581923755742571\n",
      "Epoch Count: 6 Average Error:  0.21430310538654673\n",
      "error increased by  0.006534392050728144\n",
      "Epoch Count: 7 Average Error:  0.21918965891618028\n",
      "error increased by  0.004886553529633547\n",
      "Epoch Count: 8 Average Error:  0.22273831046492037\n",
      "error increased by  0.003548651548740095\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-f9bc94389f35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw1bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneural_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhog_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-137-57a515a30837>\u001b[0m in \u001b[0;36mneural_net\u001b[0;34m(network, out, hidden)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mw2\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0maplha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdel_layer2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mb2\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0maplha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdel_bias_layer2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0mw1\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0maplha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdel_layer1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mb1\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0maplha\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdel_bias_layer1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mep_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_sq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w1,w1bias,w2,w2bias = neural_net(np.array(hog_train),np.array(training_output),200)\n",
    "predicted_output=predict(w1,w1bias,w2,w2bias,np.array(hog_test).reshape(10,7524))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_output=predict(w1,w1bias,w2,w2bias,np.array(hog_test).reshape(10,7524))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.24379606]]\n",
      "[[0.20205781]]\n",
      "[[0.11692276]]\n",
      "[[0.04390592]]\n",
      "[[0.57266771]]\n",
      "[[0.3434412]]\n",
      "[[0.94416747]]\n",
      "[[0.95412452]]\n",
      "[[0.61375254]]\n",
      "[[0.80987334]]\n",
      "10\n",
      "correct = 8\n",
      "wrong = 2\n",
      "[0, 0, 0, 0, 1, 0, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "pre=[]\n",
    "\n",
    "for check in predicted_output:\n",
    "    if(check >=0.5):\n",
    "        pre.append(1)\n",
    "    else:\n",
    "        pre.append(0)\n",
    "    print(check)\n",
    "\n",
    "print(len(pre))\n",
    "\n",
    "correct=0\n",
    "wrong=0\n",
    "\n",
    "for i in range(len(pre)):\n",
    "    if(pre[i]==testing_output[i]):\n",
    "        correct+=1\n",
    "    else:\n",
    "        wrong+=1\n",
    "\n",
    "print('correct = %d'%(correct))\n",
    "print('wrong = %d'%(wrong))\n",
    "\n",
    "print(pre)\n",
    "print(testing_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = HOGFeature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_train, lbp_train = h.HogLbp(training_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_test, lbp_test = h.HogLbp(testing_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_lbp_train = []\n",
    "for i,j in zip(hog_train, lbp_train):\n",
    "    hog_lbp_train.append(np.append(i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_lbp_test = []\n",
    "for i,j in zip(hog_test, lbp_test):\n",
    "    hog_lbp_test.append(np.append(i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_hog = np.array(hog_train)\n",
    "test_data_hog = np.array(hog_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_hoglbp = np.array(hog_lbp_train)\n",
    "test_data_hoglbp = np.array(hog_lbp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hog_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Count: 0 Average Error:  0.4739110334015451\n",
      "error decreased by  9.223372036854776e+18\n",
      "Epoch Count: 1 Average Error:  0.4999999064185429\n",
      "error increased by  0.026088873016997804\n",
      "Epoch Count: 2 Average Error:  0.4999999063908094\n",
      "error decreased by  2.7733537688590104e-11\n",
      "Epoch Count: 3 Average Error:  0.4999999063630588\n",
      "error decreased by  2.77505796120181e-11\n",
      "training complete....\n",
      "image:  ImageData/Test_images_Neg/no_person__no_bike_264_cut.bmp\n",
      "Predicted Probability: 7.144346775614619e-13 Actual Probability Value: 0\n",
      "image:  ImageData/Test_images_Neg/00000118a_cut.bmp\n",
      "Predicted Probability: 5.682843184886223e-16 Actual Probability Value: 0\n",
      "image:  ImageData/Test_images_Neg/00000003a_cut.bmp\n",
      "Predicted Probability: 6.045730773300045e-14 Actual Probability Value: 0\n",
      "image:  ImageData/Test_images_Neg/00000090a_cut.bmp\n",
      "Predicted Probability: 1.105510291327582e-18 Actual Probability Value: 0\n",
      "image:  ImageData/Test_images_Neg/no_person__no_bike_258_Cut.bmp\n",
      "Predicted Probability: 0.00013310205729939918 Actual Probability Value: 0\n",
      "image:  ImageData/Test_images_Pos/crop001034b.bmp\n",
      "Predicted Probability: 3.6764925565367963e-17 Actual Probability Value: 1\n",
      "misclassified!!!!!!!!!!!!!!!!\n",
      "image:  ImageData/Test_images_Pos/person_and_bike_151a.bmp\n",
      "Predicted Probability: 1.34430916961954e-06 Actual Probability Value: 1\n",
      "misclassified!!!!!!!!!!!!!!!!\n",
      "image:  ImageData/Test_images_Pos/crop001278a.bmp\n",
      "Predicted Probability: 5.401302915817913e-15 Actual Probability Value: 1\n",
      "misclassified!!!!!!!!!!!!!!!!\n",
      "image:  ImageData/Test_images_Pos/crop001500b.bmp\n",
      "Predicted Probability: 1.8728022816480874e-15 Actual Probability Value: 1\n",
      "misclassified!!!!!!!!!!!!!!!!\n",
      "image:  ImageData/Test_images_Pos/crop001070a.bmp\n",
      "Predicted Probability: 2.961213585705013e-10 Actual Probability Value: 1\n",
      "misclassified!!!!!!!!!!!!!!!!\n",
      "50.0 % Prediction Accuracy\n"
     ]
    }
   ],
   "source": [
    "#initialize the neural network\n",
    "neuralNet = NeuralNet(graph = (7524, 200, 1), ep = 100, lr = 0.1, patience = 3)\n",
    "neuralNet.train(train_data_hog, training_output)\n",
    "neuralNet.test(testing_image_paths, test_data_hog, testing_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate RELU\n",
    "\n",
    "\n",
    "def relu(x):\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            if x[i,j]<0:\n",
    "                x[i,j]=0\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to calculate sigmoid\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to implament neural network, to train it.\n",
    "\n",
    "def neural_net(inp,out,hidden):\n",
    "    aplha = 0.1                                               # Initializing the learning rate\n",
    "    col = inp.shape[1]                                         \n",
    "    col2 = 1\n",
    "    w1 = np.random.randn(col,hidden)                          # Weight for layer 1\n",
    "    w1 = np.multiply(w1,m.sqrt(2/int(col+hidden)))            # Faactoring the weight\n",
    "    w2 = np.random.randn(hidden,col2)                         # Weight for layer 2\n",
    "    w2 = np.multiply(w2,m.sqrt(2/int(hidden+col2)))           # Factoring the weight\n",
    "    w1bias = np.random.randn(hidden)                          # Bias for layer 1\n",
    "    w1bias = np.multiply(w1bias,m.sqrt(2/int(hidden)))\n",
    "    w2bias = np.random.randn(col2)                           # Bias for layer 2\n",
    "    w2bias = np.multiply(w2bias,m.sqrt(2/int(col2)))\n",
    "    err_curve=np.zeros((100,1))                              # Error array for each epoch\n",
    "    epoch = 0\n",
    "    while epoch<100:                                         # Doing forward and backward propogation for each epoch\n",
    "        for i in range(inp.shape[0]):\n",
    "            x = inp[i,:].reshape([1,-1])\n",
    "            z = relu((x.dot(w1)+w1bias))                     # Computing values for hidden layer\n",
    "            y = sigmoid((z.dot(w2)+w2bias))                  # Computing values for output layer\n",
    "            err = out[i]-y                                   # Error for output layer             \n",
    "            sqerr = 0.5*err*err                              # Square error                      \n",
    "            del_out=(-1*err)*(1-y)*y                         \n",
    "            del_layer2=z.T.dot(del_out)\n",
    "            del_layer20=np.sum(del_out,axis=0)\n",
    "            zz=np.zeros_like(z)\n",
    "            for k in range(hidden):\n",
    "            \n",
    "                if(z[0][k]>0):\n",
    "                    zz[0][k]=1\n",
    "                else:\n",
    "                    zz[0][k]=0                       \n",
    "            del_hidden= del_out.dot(w2.T)*zz\n",
    "            del_layer1=x.T.dot(del_hidden)\n",
    "            delta_layer10=np.sum(del_hidden,axis=0)\n",
    "            \n",
    "            w2-= aplha*del_layer2\n",
    "            w2bias-= aplha*del_layer20\n",
    "            w1-= aplha*del_layer1\n",
    "            w1bias-= aplha*delta_layer10\n",
    "            err_curve[epoch] = sqerr/inp.shape[0]\n",
    "        print('Epoch %d: err %f'%(epoch,np.mean(sqerr)/inp.shape[0]))\n",
    "        epoch +=1\n",
    "    return w1,w1bias,w2,w2bias,err_curve\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to predict values for my neural network\n",
    "\n",
    "def predict(w,wb,v,vb,Output_descriptor):\n",
    "    Number_of_test_image,number_of_attribute=Output_descriptor.shape\n",
    "    predict=[]\n",
    "    for k in range(Number_of_test_image):\n",
    "            x=Output_descriptor[k,:].reshape([1,-1])\n",
    "            z=relu((x.dot(w)+wb))\n",
    "            y=sigmoid(z.dot(v)+vb)\n",
    "            predict.append(y)\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.5708519 , 0.33140353, 0.10557453, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.09737358, 0.09502576, 0.07123331, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.03649368, 0.03286175, 0.0394938 , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.07888919, 0.21040417, 0.28637606, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00648847, 0.00526827, 0.00542789, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.18969883, 0.03537474, 0.00214704, ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(hog_test).reshape(10,7524)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Count: 0 Average Error:  0.06289534608805206\n",
      "error decreased by  9.223372036854776e+18\n",
      "Epoch Count: 1 Average Error:  0.11781752996241827\n",
      "error increased by  0.05492218387436622\n",
      "Epoch Count: 2 Average Error:  0.1408547212669945\n",
      "error increased by  0.023037191304576224\n",
      "Epoch Count: 3 Average Error:  0.1560235579453531\n",
      "error increased by  0.01516883667835861\n",
      "Epoch Count: 4 Average Error:  0.167212648562014\n",
      "error increased by  0.011189090616660902\n",
      "Epoch Count: 5 Average Error:  0.17641659325442902\n",
      "error increased by  0.009203944692415006\n",
      "Epoch Count: 6 Average Error:  0.18310753052234852\n",
      "error increased by  0.0066909372679195\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-c0f749e133bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw1bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merr_curve\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneural_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhog_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mpredicted_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw1bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhog_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7524\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-127-c1a3d29d689b>\u001b[0m in \u001b[0;36mneural_net\u001b[0;34m(network, out, hidden)\u001b[0m\n\u001b[1;32m     57\u001b[0m                     \u001b[0mlayer1_output_like\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mdel_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdel_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlayer1_output_like\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mdel_layer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdel_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m             \u001b[0mdel_bias_layer1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdel_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "w1,w1bias,w2,w2bias,err_curve = neural_net(np.array(hog_train),np.array(training_output),200)\n",
    "\n",
    "predicted_output=predict(w1,w1bias,w2,w2bias,np.array(hog_test).reshape(10,7524))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.random((200, 7524)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2 = np.random.random((1, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 200)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = 0.1 * (2 * np.random.random((200, 7524)) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 7524)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (200,200) and (11064,) not aligned: 200 (dim 1) != 11064 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-4e55a00e0e25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#initialize the neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mneuralNet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m11064\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mneuralNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_hoglbp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mneuralNet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting_image_paths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data_hoglbp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-f7b0c4e5a79a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_data, label)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0mep_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mep_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mep_err\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mdataLen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-86-f7b0c4e5a79a>\u001b[0m in \u001b[0;36mbp\u001b[0;34m(self, td, expedted_output)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_w1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_b2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (200,200) and (11064,) not aligned: 200 (dim 1) != 11064 (dim 0)"
     ]
    }
   ],
   "source": [
    "#initialize the neural network\n",
    "neuralNet = NeuralNet(graph = (11064, 200, 1), ep = 100, lr = 0.1, patience = 3)\n",
    "neuralNet.train(train_data_hoglbp, training_output)\n",
    "neuralNet.test(testing_image_paths, test_data_hoglbp, testing_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"HumanClassifier.py\", line 394, in <module>\n",
      "    test_data_input = np.array(h.hog(test_image_path_list))\n",
      "  File \"HumanClassifier.py\", line 177, in hog\n",
      "    img = self.bgr2gray(img)\n",
      "  File \"HumanClassifier.py\", line 17, in bgr2gray\n",
      "    rows = img.shape[0]\n",
      "AttributeError: 'NoneType' object has no attribute 'shape'\n"
     ]
    }
   ],
   "source": [
    "!python HumanClassifier.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "!find . -name \"*.txt\" -type f -delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ImageData/Test\\ images\\ \\(Pos\\)/_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r ImageData/*_res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
