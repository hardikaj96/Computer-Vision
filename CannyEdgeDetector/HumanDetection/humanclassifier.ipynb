{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import cv2\n",
    "import math\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getImagePathsWithOutput(path_dir_output):\n",
    "    image_paths = []\n",
    "    output = []\n",
    "    for folder in path_dir_output.keys():\n",
    "        for dirt, subdirt, fileList in os.walk(folder):\n",
    "            for file in fileList:\n",
    "                image_paths.append(folder + file)\n",
    "                output.append(path_dir_output[folder])\n",
    "    return image_paths, output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main\n",
    "root = 'ImageData/' \n",
    "train_pos_dir = root + 'Training_images_Pos/'\n",
    "train_neg_dir = root + 'Training_images_Neg/' \n",
    "test_pos_dir = root + 'Test_images_Pos/' \n",
    "test_neg_dir = root + 'Test_images_Neg/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir_with_output = {\n",
    "    train_pos_dir:1,\n",
    "    train_neg_dir:0\n",
    "}\n",
    "test_data_dir_with_output = {\n",
    "    test_neg_dir:0,\n",
    "    test_pos_dir:1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_image_paths, training_output = getImagePathsWithOutput(train_data_dir_with_output)\n",
    "testing_image_paths, testing_output = getImagePathsWithOutput(test_data_dir_with_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HOGFeature:\n",
    "    \n",
    "    def rgb2gray(self, img):\n",
    "        '''\n",
    "        function to convert image from color image to grayscale image\n",
    "        img = the image matrix\n",
    "        '''\n",
    "        gray = np.zeros((img.shape[0],img.shape[1]))\n",
    "        for i in range(0,img.shape[0]):\n",
    "            for j in range(0,img.shape[1]):\n",
    "                bgr = img[i,j]\n",
    "                b = bgr[0]\n",
    "                g = bgr[1]\n",
    "                r = bgr[2]\n",
    "                gray[i,j] = round(0.299*r+0.587*g+0.114*b)\n",
    "        return gray\n",
    "    \n",
    "    def apply_sobel(self, img):\n",
    "        '''\n",
    "        Function to compute Normalized Gradient Magnitude and Gradient Angle\n",
    "        '''\n",
    "        #Sobel Filter Mask to calculate Horizontal(x) gradient\n",
    "        sobel_x = (1/4)*np.array([(-1,0,1),\n",
    "                    (-2,0,2),\n",
    "                    (-1,0,1)])\n",
    "        #Sobel Filter Mask to calculate Vertical(y) gradient\n",
    "        sobel_y = (1/4)*np.array([(1,2,1),\n",
    "                    (0,0,0),\n",
    "                    (-1,-2,-1)])\n",
    "\n",
    "        #initialize matrices to store the value of horizontal and vertical gradient,\n",
    "        #normalized horizontal and vertical gradient, normalized gradient magnitude\n",
    "        #and gradient angle\n",
    "        gradient_magnitude = np.zeros(shape=img.shape)\n",
    "        gradient_angle = np.zeros(shape=img.shape)\n",
    "        #find the gradient values by perfoeming convolution\n",
    "        for row in range(0,img.shape[0]-2):\n",
    "            for col in range(0,img[row].size-2):\n",
    "                #calculate Value at current pixel (row,col) \n",
    "                #after applying sobel operator\n",
    "                gx, gy = 0, 0\n",
    "                for i in range (0,3):\n",
    "                    for j in range (0,3):\n",
    "                        gx = gx + img[row+i][col+j] * sobel_x[i][j]\n",
    "                        gy = gy + img[row+i][col+j] * sobel_y[i][j]\n",
    "                #normalize gradient magnitude by dividing by sqrt(2)\n",
    "                gradient_magnitude[row+1][col+1]=((gx**2+gy**2)**(0.5))/(1.4142)\n",
    "                #calculate gradient angle based on sobel horizontal gradient and vertical gradient\n",
    "                angle = 0\n",
    "                if(gx == 0):\n",
    "                    if( gy > 0):\n",
    "                        angle = 90\n",
    "                    else:\n",
    "                        angle = -90\n",
    "                else:\n",
    "                    angle = math.degrees(math.atan(gy/gx))\n",
    "                if (angle < 0):\n",
    "                    angle = angle + 180\n",
    "                gradient_angle[row+1,col+1]  = angle\n",
    "        return [gradient_magnitude, gradient_angle]\n",
    "    \n",
    "    def get_cell_histogram(self, gradient_magnitude, gradient_angle):\n",
    "        '''\n",
    "        This function is to get histogram for each 8x8 cell\n",
    "        gradient_magnitude = gradient magnitude for each pixel\n",
    "        gradient_angle = gradient angle for each pixel\n",
    "        '''\n",
    "        cell_shape = gradient_magnitude.shape\n",
    "        #initialize the number of cell rows and cell columns\n",
    "        cell_rows = round(cell_shape[0]/8)\n",
    "        cell_cols = round(cell_shape[1]/8)\n",
    "        histogram_cell = np.zeros((cell_rows,cell_cols,9))\n",
    "        for r in range (0,cell_rows-1):\n",
    "            for c in range (0,cell_cols-1):\n",
    "                for row in range (r*8,r*8+8):\n",
    "                    for col in range (c*8,c*8+8):\n",
    "                        angle = gradient_angle[row][col]\n",
    "                        grad_mag = gradient_magnitude[row][col]\n",
    "                        if angle%20 == 0:\n",
    "                            if angle == 180:\n",
    "                                histogram_cell[r][c][0] += grad_mag\n",
    "                                continue\n",
    "                            bin_no = int(angle/20)\n",
    "                            histogram_cell[r][c][bin_no] += grad_mag\n",
    "                            continue\n",
    "                        bin_no_l = int(angle/20)\n",
    "                        #calculate the vote for left and right bins.\n",
    "                        if bin_no_l < 8:\n",
    "                            bin_no_r = bin_no_l + 1\n",
    "                            histogram_cell[r][c][bin_no_r] += grad_mag*((angle - (bin_no_l * 20))/20)\n",
    "                            histogram_cell[r][c][bin_no_l] += grad_mag*(((bin_no_r * 20) - angle)/20)\n",
    "                        else:\n",
    "                            bin_no_r = 0\n",
    "                            histogram_cell[r][c][bin_no_r] += grad_mag*((angle - 160)/20)\n",
    "                            histogram_cell[r][c][bin_no_l] += grad_mag*((180 - angle)/20)\n",
    "        squared_histogram_cell = np.square(histogram_cell)\n",
    "        return [histogram_cell, squared_histogram_cell] \n",
    " \n",
    "    \n",
    "    def get_hog_descriptor(self, histogram_cell, histogram_cell_squared):\n",
    "        cell_histogram_shape = histogram_cell.shape\n",
    "        descriptor = np.array([])\n",
    "        for row in range(0,cell_histogram_shape[0]-1):\n",
    "            for col in range(0,cell_histogram_shape[1]-1):\n",
    "                block = np.array([])\n",
    "                block_squared = np.array([])\n",
    "                block = np.append(block,histogram_cell[row,col])\n",
    "                block = np.append(block,histogram_cell[row,col+1])\n",
    "                block = np.append(block,histogram_cell[row+1,col])\n",
    "                block = np.append(block,histogram_cell[row+1,col+1])\n",
    "                block_squared = np.append(block_squared,histogram_cell_squared[row,col])\n",
    "                block_squared = np.append(block_squared,histogram_cell_squared[row,col+1])\n",
    "                block_squared = np.append(block_squared,histogram_cell_squared[row+1,col])\n",
    "                block_squared = np.append(block_squared,histogram_cell_squared[row+1,col+1])\n",
    "                block_squared = np.sum(block_squared)\n",
    "                if(block_squared>0):\n",
    "                    #normalize the block descriptor\n",
    "                    normalized = np.sqrt(block_squared)\n",
    "                    block = (1/normalized)*block\n",
    "                descriptor = np.append(descriptor, block)\n",
    "        return descriptor\n",
    "\n",
    "    \n",
    "    def get_LBP_descriptor(self, magnitude):\n",
    "        '''\n",
    "        This function is to get histogram for each 8x8 cell\n",
    "        gradient_magnitude = gradient magnitude for each pixel\n",
    "        gradient_angle = gradient angle for each pixel\n",
    "        '''\n",
    "        pattern = [(-1, -1),(-1, 0),(-1, 1),(0, -1),(0, 1),(1, -1),(1, 0),(1, 1)]\n",
    "        uniform_patterns = np.array([0, 1, 2, 3, 4, 6, 7, 8, 12, 14, 15, 16, 24, 28, 30, 31, 32, 48, 56,\n",
    "        60, 62, 63, 64, 96, 112, 120, 124, 126, 127, 128, 129, 131, 135, 143, 159, 191, 192, 193, 195, 199,\n",
    "        207, 223, 224, 225, 227, 231, 239, 240, 241, 243, 247, 248, 249, 251, 252, 253, 254, 255])\n",
    "        non_uniform_patterns = np.array(list(set(np.arange(256))-set(uniform_patterns)))\n",
    "        cell_shape = magnitude.shape\n",
    "        #initialize the number of cell rows and cell columns\n",
    "        cell_rows = round(cell_shape[0]/16)\n",
    "        cell_cols = round(cell_shape[1]/16)\n",
    "        histogram_block = np.zeros((cell_rows,cell_cols,59))\n",
    "        lbp_mask = np.zeros(magnitude.shape)\n",
    "        lbp_descriptor = np.array([])\n",
    "        for r in range (0,cell_rows):\n",
    "            for c in range (0,cell_cols):\n",
    "                histogram_block = np.zeros(256)\n",
    "                for row in range (r*16,r*16+16):\n",
    "                    for col in range (c*16,c*16+16):\n",
    "                        if row == 0 or col == 0 or row == magnitude.shape[0]-1 or col == magnitude.shape[1]-1:\n",
    "                            lbp_mask[row][col] = 5\n",
    "                            continue\n",
    "                        bin_val = ''\n",
    "                        mag = magnitude[row][col]\n",
    "                        for i,j in pattern:\n",
    "                            if magnitude[row+i][col+j] >= mag:\n",
    "                                bin_val += '1'\n",
    "                            else:\n",
    "                                bin_val += '0'\n",
    "                        histogram_block[int(bin_val,2)] += 1\n",
    "                normalized_histogram = histogram_block/256\n",
    "                bin_59 = sum(normalized_histogram[non_uniform_patterns])\n",
    "                bin_1_58 = normalized_histogram[uniform_patterns]\n",
    "                bin_1_59 = np.append(bin_1_58, bin_59)\n",
    "                lbp_descriptor = np.append(lbp_descriptor,bin_1_59)\n",
    "        return lbp_descriptor\n",
    "    \n",
    "    \n",
    "    def saveImageAndHog(self, mag, hog, lbp, path):\n",
    "        \"\"\"\n",
    "        function to save the image and hog descriptor\n",
    "        \"\"\"\n",
    "        pathSplit = path.split('/')\n",
    "        currImage = pathSplit[-1]\n",
    "        imageName, imageExt = currImage.split('.')\n",
    "        updatedImage = '.'.join([imageName+'_mag',imageExt])\n",
    "        imageFolder = '/'.join(pathSplit[:-1])\n",
    "        imageFolder += \"_res\"\n",
    "        if not os.path.exists(imageFolder):\n",
    "            os.makedirs(imageFolder)\n",
    "        finalPath = '/'.join([imageFolder,updatedImage])\n",
    "        cv2.imwrite(finalPath, mag)\n",
    "        filename = imageFolder+\"/\"+imageName+\"_hog.txt\"\n",
    "        hog_file = open(filename,'a+')\n",
    "        for i in hog:\n",
    "            hog_file.write(str(i[0])+'\\n')\n",
    "        hog_file.close()\n",
    "        filename = imageFolder+\"/\"+imageName+\"_lbp.txt\"\n",
    "        lbp_file = open(filename,'a+')\n",
    "        for i in lbp:\n",
    "            lbp_file.write(str(i[0])+'\\n')\n",
    "        lbp_file.close()\n",
    "        \n",
    "    def HogLbp(self, image_list):\n",
    "        hog_features = []\n",
    "        lbp_features = []\n",
    "        for image in image_list:\n",
    "            color_img = cv2.imread(image,cv2.IMREAD_COLOR)\n",
    "            gray_img = self.rgb2gray(color_img)\n",
    "            gradient_magnitude, gradient_angle = self.apply_sobel(gray_img)\n",
    "            histogram = self.get_cell_histogram(gradient_magnitude, gradient_angle)\n",
    "            histogram_cell = histogram[0]\n",
    "            squared_histogram_cell = histogram[1]\n",
    "            HOG_descriptor = self.get_hog_descriptor(histogram_cell, squared_histogram_cell)\n",
    "            HOG_descriptor = HOG_descriptor.reshape(-1,1)\n",
    "            LBP_descriptor = self.get_LBP_descriptor(gradient_magnitude)\n",
    "            LBP_descriptor = LBP_descriptor.reshape(-1,1)\n",
    "            self.saveImageAndHog(gradient_magnitude, HOG_descriptor, LBP_descriptor, image)\n",
    "            hog_features.append(HOG_descriptor)\n",
    "            lbp_features.append(LBP_descriptor)\n",
    "        return hog_features,lbp_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    \"\"\"Function to calculate RELU\n",
    "    \"\"\"\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            if x[i,j]<0:\n",
    "                x[i,j]=0\n",
    "    return x\n",
    "\n",
    "def Sigmoid(x):\n",
    "    \"\"\"\n",
    "    Function to calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def neural_network(network,out,hidden):\n",
    "    \"\"\"\n",
    "    Function to implament neural network, to train it.\n",
    "    \"\"\"\n",
    "    aplha = 0.1                                               \n",
    "    # Initializing the learning rate\n",
    "    total_hidden = network.shape[1]                                         \n",
    "    total_output = network.shape[2]\n",
    "    # Initializing and Factoring the weight for hidden layer\n",
    "    w1 = np.random.randn(total_hidden,hidden)                          \n",
    "    w1 = np.multiply(w1,math.sqrt(2/int(total_hidden+hidden)))\n",
    "    # Initializing and Factoring the weight for output layer         \n",
    "    w2 = np.random.randn(hidden,total_output)\n",
    "    w2 = np.multiply(w2,math.sqrt(2/int(hidden+total_output)))\n",
    "    # Bias for hidden and output layer\n",
    "    b1 = np.random.randn(hidden)                         \n",
    "    b1 = np.multiply(b1,math.sqrt(2/int(hidden)))\n",
    "    b2 = np.random.randn(total_output)\n",
    "    b2 = np.multiply(b2,math.sqrt(2/int(total_output)))\n",
    "    epoch = 0\n",
    "    err_sq = 0\n",
    "    prev_err = sys.maxsize\n",
    "    while True:                                         \n",
    "        # FeedForward and Backpropagation for each epoch of all vectors\n",
    "        for i in range(network.shape[0]):\n",
    "            x = network[i,:].reshape([1,-1])                     \n",
    "            # Computing values for hidden layer and output layer in feedforward\n",
    "            layer1_output = ReLU((x.dot(w1)+b1))\n",
    "            layer2_output = Sigmoid((layer1_output.dot(w2)+b2))\n",
    "            err = out[i]-layer2_output           \n",
    "            err_sq += 0.5*err*err\n",
    "            #Doing BackPropagation\n",
    "            del_output = (-1*err)*(1-layer2_output)*layer2_output                         \n",
    "            del_layer2 = layer1_output.T.dot(del_output)\n",
    "            del_bias_layer2 = np.sum(del_output,axis = 0)\n",
    "            layer1_output_like = np.zeros_like(layer1_output)\n",
    "            for k in range(hidden):\n",
    "                if(layer1_output[0][k]>0):\n",
    "                    layer1_output_like[0][k] = 1\n",
    "                else:\n",
    "                    layer1_output_like[0][k] = 0                       \n",
    "            del_hidden = del_output.dot(w2.T)*layer1_output_like\n",
    "            del_layer1 = x.T.dot(del_hidden)\n",
    "            del_bias_layer1 = np.sum(del_hidden,axis=0)\n",
    "\n",
    "            w2 -= aplha*del_layer2\n",
    "            b2 -= aplha*del_bias_layer2\n",
    "            w1 -= aplha*del_layer1\n",
    "            b1 -= aplha*del_bias_layer1\n",
    "        ep_err = np.mean(err_sq)/network.shape[0]\n",
    "        print(\"Epoch Count: \" + str(epoch), \"Average Error: \", ep_err)\n",
    "        if(ep_err < prev_err):\n",
    "            print(\"error decreased by \", prev_err-ep_err)\n",
    "        else:\n",
    "            if(ep_err > prev_err):\n",
    "                print(\"error increased by \", ep_err-prev_err)\n",
    "            else:\n",
    "                print(\"error stayed same\")\n",
    "        #check for the change in error if very less we can stop training\n",
    "        if(abs(prev_err - ep_err) < 0.0001):\n",
    "            print(\"training complete....\")\n",
    "            break\n",
    "        prev_err = ep_err\n",
    "        epoch += 1\n",
    "\n",
    "    return w1,b1,w2,b2\n",
    "\n",
    "def predict(w,wb,v,vb,output_descriptor):\n",
    "    # Function to predict values for my neural network\n",
    "    number_of_test_image,number_of_attribute=output_descriptor.shape\n",
    "    predict=[]\n",
    "    for k in range(number_of_test_image):\n",
    "            x=output_descriptor[k,:].reshape([1,-1])\n",
    "            z=ReLU((x.dot(w)+wb))\n",
    "            y=Sigmoid(z.dot(v)+vb)\n",
    "            predict.append(y)\n",
    "    return predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = HOGFeature()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_train, lbp_train = h.HogLbp(training_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_test, lbp_test = h.HogLbp(testing_image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_lbp_train = []\n",
    "for i,j in zip(hog_train, lbp_train):\n",
    "    hog_lbp_train.append(np.append(i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "hog_lbp_test = []\n",
    "for i,j in zip(hog_test, lbp_test):\n",
    "    hog_lbp_test.append(np.append(i,j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_hog = np.array(hog_train)\n",
    "test_data_hog = np.array(hog_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_hoglbp = np.array(hog_lbp_train).reshape(20,11064,1)\n",
    "test_data_hoglbp = np.array(hog_lbp_test).reshape(10,11064,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HIDDEN LAYER = 200 \n",
      "\n",
      "\n",
      "Epoch Count: 0 Average Error:  0.0722127435571853\n",
      "error decreased by  9.223372036854776e+18\n",
      "Epoch Count: 1 Average Error:  0.11546090607808143\n",
      "error increased by  0.04324816252089614\n",
      "Epoch Count: 2 Average Error:  0.13834471535841084\n",
      "error increased by  0.022883809280329404\n",
      "Epoch Count: 3 Average Error:  0.15346050767510658\n",
      "error increased by  0.015115792316695742\n",
      "Epoch Count: 4 Average Error:  0.1645564948723161\n",
      "error increased by  0.011095987197209528\n",
      "Epoch Count: 5 Average Error:  0.17367965830845558\n",
      "error increased by  0.009123163436139475\n",
      "Epoch Count: 6 Average Error:  0.18017517978734712\n",
      "error increased by  0.006495521478891542\n",
      "Epoch Count: 7 Average Error:  0.1848338318157149\n",
      "error increased by  0.004658652028367766\n",
      "Epoch Count: 8 Average Error:  0.18796851482975097\n",
      "error increased by  0.003134683014036077\n",
      "Epoch Count: 9 Average Error:  0.19004049227140493\n",
      "error increased by  0.002071977441653966\n",
      "Epoch Count: 10 Average Error:  0.1915096819462047\n",
      "error increased by  0.0014691896747997812\n",
      "Epoch Count: 11 Average Error:  0.19265177829072924\n",
      "error increased by  0.0011420963445245291\n",
      "Epoch Count: 12 Average Error:  0.1935939154947356\n",
      "error increased by  0.0009421372040063491\n",
      "Epoch Count: 13 Average Error:  0.19439871559364647\n",
      "error increased by  0.0008048000989108739\n",
      "Epoch Count: 14 Average Error:  0.19510179294475405\n",
      "error increased by  0.0007030773511075838\n",
      "Epoch Count: 15 Average Error:  0.19572882340366377\n",
      "error increased by  0.0006270304589097164\n",
      "Epoch Count: 16 Average Error:  0.19629420007735437\n",
      "error increased by  0.0005653766736906096\n",
      "Epoch Count: 17 Average Error:  0.19680907304412634\n",
      "error increased by  0.0005148729667719676\n",
      "Epoch Count: 18 Average Error:  0.19728287409014875\n",
      "error increased by  0.000473801046022404\n",
      "Epoch Count: 19 Average Error:  0.19772033961777966\n",
      "error increased by  0.0004374655276309092\n",
      "Epoch Count: 20 Average Error:  0.19812755496988058\n",
      "error increased by  0.0004072153521009203\n",
      "Epoch Count: 21 Average Error:  0.19850877371370979\n",
      "error increased by  0.00038121874382920917\n",
      "Epoch Count: 22 Average Error:  0.1988669763396052\n",
      "error increased by  0.00035820262589542073\n",
      "Epoch Count: 23 Average Error:  0.19920405906558805\n",
      "error increased by  0.0003370827259828457\n",
      "Epoch Count: 24 Average Error:  0.19952279698625958\n",
      "error increased by  0.0003187379206715246\n",
      "Epoch Count: 25 Average Error:  0.1998256703258298\n",
      "error increased by  0.00030287333957021434\n",
      "Epoch Count: 26 Average Error:  0.20011337711167426\n",
      "error increased by  0.0002877067858444715\n",
      "Epoch Count: 27 Average Error:  0.20038770737357256\n",
      "error increased by  0.0002743302618982979\n",
      "Epoch Count: 28 Average Error:  0.2006498954861105\n",
      "error increased by  0.0002621881125379477\n",
      "Epoch Count: 29 Average Error:  0.20090062157865884\n",
      "error increased by  0.0002507260925483368\n",
      "Epoch Count: 30 Average Error:  0.20114122132970896\n",
      "error increased by  0.000240599751050119\n",
      "Epoch Count: 31 Average Error:  0.2013722196959435\n",
      "error increased by  0.00023099836623452497\n",
      "Epoch Count: 32 Average Error:  0.201594574488061\n",
      "error increased by  0.0002223547921175184\n",
      "Epoch Count: 33 Average Error:  0.2018085309195627\n",
      "error increased by  0.00021395643150168286\n",
      "Epoch Count: 34 Average Error:  0.20201485902993171\n",
      "error increased by  0.0002063281103690251\n",
      "Epoch Count: 35 Average Error:  0.20221449431747093\n",
      "error increased by  0.00019963528753921822\n",
      "Epoch Count: 36 Average Error:  0.20240717203905664\n",
      "error increased by  0.00019267772158571006\n",
      "Epoch Count: 37 Average Error:  0.20259378552843194\n",
      "error increased by  0.00018661348937529243\n",
      "Epoch Count: 38 Average Error:  0.20277437897724937\n",
      "error increased by  0.00018059344881743433\n",
      "Epoch Count: 39 Average Error:  0.20294975270830587\n",
      "error increased by  0.00017537373105649956\n",
      "Epoch Count: 40 Average Error:  0.20311991440451221\n",
      "error increased by  0.00017016169620634458\n",
      "Epoch Count: 41 Average Error:  0.2032851706096709\n",
      "error increased by  0.0001652562051586981\n",
      "Epoch Count: 42 Average Error:  0.20344570875180107\n",
      "error increased by  0.0001605381421301566\n",
      "Epoch Count: 43 Average Error:  0.203601922896608\n",
      "error increased by  0.00015621414480693208\n",
      "Epoch Count: 44 Average Error:  0.20375401364316453\n",
      "error increased by  0.00015209074655653243\n",
      "Epoch Count: 45 Average Error:  0.2039021980624552\n",
      "error increased by  0.0001481844192906534\n",
      "Epoch Count: 46 Average Error:  0.2040465343650757\n",
      "error increased by  0.0001443363026205069\n",
      "Epoch Count: 47 Average Error:  0.20418744210005285\n",
      "error increased by  0.00014090773497715814\n",
      "Epoch Count: 48 Average Error:  0.20432487124603949\n",
      "error increased by  0.00013742914598663392\n",
      "Epoch Count: 49 Average Error:  0.2044591327465116\n",
      "error increased by  0.00013426150047210483\n",
      "Epoch Count: 50 Average Error:  0.20459023156167672\n",
      "error increased by  0.000131098815165126\n",
      "Epoch Count: 51 Average Error:  0.20471841508299268\n",
      "error increased by  0.00012818352131596655\n",
      "Epoch Count: 52 Average Error:  0.20484380682893807\n",
      "error increased by  0.00012539174594539126\n",
      "Epoch Count: 53 Average Error:  0.2049664011342661\n",
      "error increased by  0.00012259430532801652\n",
      "Epoch Count: 54 Average Error:  0.20508644291359338\n",
      "error increased by  0.00012004177932728455\n",
      "Epoch Count: 55 Average Error:  0.20520403271029047\n",
      "error increased by  0.00011758979669709846\n",
      "Epoch Count: 56 Average Error:  0.2053191515447052\n",
      "error increased by  0.00011511883441472914\n",
      "Epoch Count: 57 Average Error:  0.2054319988724239\n",
      "error increased by  0.00011284732771868478\n",
      "Epoch Count: 58 Average Error:  0.20554258711098133\n",
      "error increased by  0.00011058823855744637\n",
      "Epoch Count: 59 Average Error:  0.20565111320104817\n",
      "error increased by  0.00010852609006684055\n",
      "Epoch Count: 60 Average Error:  0.2057575993962118\n",
      "error increased by  0.00010648619516362423\n",
      "Epoch Count: 61 Average Error:  0.20586206440319685\n",
      "error increased by  0.00010446500698504946\n",
      "Epoch Count: 62 Average Error:  0.20596460888232299\n",
      "error increased by  0.00010254447912613762\n",
      "Epoch Count: 63 Average Error:  0.20606535336595502\n",
      "error increased by  0.00010074448363203703\n",
      "Epoch Count: 64 Average Error:  0.2061642690298772\n",
      "error increased by  9.891566392217221e-05\n",
      "training complete....\n",
      "ImageData/Test_images_Neg/no_person__no_bike_264_cut.bmp = no-human with predicted value 0.2918207736828984\n",
      "ImageData/Test_images_Neg/00000118a_cut.bmp = no-human with predicted value 0.15065646415972522\n",
      "ImageData/Test_images_Neg/00000003a_cut.bmp = no-human with predicted value 0.1457784506633768\n",
      "ImageData/Test_images_Neg/00000090a_cut.bmp = no-human with predicted value 0.03432886178902997\n",
      "ImageData/Test_images_Neg/no_person__no_bike_258_Cut.bmp = human with predicted value 0.6041145283961911\n",
      "ImageData/Test_images_Pos/crop001034b.bmp = no-human with predicted value 0.29714946469454634\n",
      "ImageData/Test_images_Pos/person_and_bike_151a.bmp = human with predicted value 0.9559860944955897\n",
      "ImageData/Test_images_Pos/crop001278a.bmp = human with predicted value 0.956580685317689\n",
      "ImageData/Test_images_Pos/crop001500b.bmp = human with predicted value 0.639633749783504\n",
      "ImageData/Test_images_Pos/crop001070a.bmp = human with predicted value 0.8589011961035388\n",
      "correct = 8\n",
      "wrong = 2\n",
      "average error = 0\n",
      "[array([[0.29182077]]), array([[0.15065646]]), array([[0.14577845]]), array([[0.03432886]]), array([[0.60411453]]), array([[0.29714946]]), array([[0.95598609]]), array([[0.95658069]]), array([[0.63963375]]), array([[0.8589012]])]\n",
      "[0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "for hidden in [200,400]:\n",
    "    print('HIDDEN LAYER = %d \\n\\n'%(hidden))\n",
    "    w1,w1bias,w2,w2bias = neural_network(np.array(hog_train),np.array(training_output),hidden)\n",
    "    predicted_output = predict(w1,w1bias,w2,w2bias,np.array(hog_test).reshape(10,7524))\n",
    "    prediction = []\n",
    "    classes = []\n",
    "    for predicted in predicted_output:\n",
    "        if(predicted >=0.5):\n",
    "            prediction.append(1)\n",
    "        else:\n",
    "            prediction.append(0)\n",
    "        if predicted >= 0.6:\n",
    "            classes.append('human')\n",
    "        elif predicted <= 0.4:\n",
    "            classes.append('no-human')\n",
    "        else:\n",
    "            classes.append('borderline')\n",
    "\n",
    "    correct=0\n",
    "    wrong=0\n",
    "    error = 0\n",
    "    for i in range(len(prediction)):\n",
    "        error += abs(testing_output[i] - predicted_output[i])\n",
    "        if(prediction[i]==testing_output[i]):\n",
    "            correct+=1\n",
    "        else:\n",
    "            wrong+=1\n",
    "        print(testing_image_paths[i] + ' = class (' + classes[i] + ') with predicted value as  :: ' + str(predicted_output[i][0][0]))\n",
    "    print('correct = %d'%(correct))\n",
    "    print('wrong = %d'%(wrong))\n",
    "    print('average error = %d'%(error/10))\n",
    "    print(predicted_output)\n",
    "    print(testing_output)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Count: 0 Average Error:  0.09399321346884207\n",
      "error decreased by  9.223372036854776e+18\n",
      "Epoch Count: 1 Average Error:  0.1495398054986695\n",
      "error increased by  0.055546592029827424\n",
      "Epoch Count: 2 Average Error:  0.17752018863608754\n",
      "error increased by  0.027980383137418047\n",
      "Epoch Count: 3 Average Error:  0.1938429927337434\n",
      "error increased by  0.016322804097655852\n",
      "Epoch Count: 4 Average Error:  0.20595993109861893\n",
      "error increased by  0.01211693836487554\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-e734c32027ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw1bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneural_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_hoglbp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mpredicted_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw1bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw2bias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_data_hoglbp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11064\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-f2a6de8b3213>\u001b[0m in \u001b[0;36mneural_network\u001b[0;34m(network, out, hidden)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;31m# Computing values for hidden layer and output layer in feedforward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mlayer1_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m             \u001b[0mlayer2_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer1_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mb2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0merr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlayer2_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for hidden in [200,400]:\n",
    "    w1,w1bias,w2,w2bias = neural_network(train_data_hoglbp,np.array(training_output),hidden)\n",
    "    predicted_output=predict(w1,w1bias,w2,w2bias,test_data_hoglbp.reshape(10,11064))\n",
    "    prediction = []\n",
    "    classes = []\n",
    "    for predicted in predicted_output:\n",
    "        if(predicted >=0.5):\n",
    "            prediction.append(1)\n",
    "        else:\n",
    "            prediction.append(0)\n",
    "        print(predicted)\n",
    "        if predicted >= 0.6:\n",
    "            classes.append('human')\n",
    "        elif predicted <= 0.4:\n",
    "            classes.append('no-human')\n",
    "        else:\n",
    "            classes.append('borderline')\n",
    "\n",
    "    correct=0\n",
    "    wrong=0\n",
    "    error = 0\n",
    "    for i in range(len(prediction)):\n",
    "        error += abs(testing_output[i] - predicted_output[i])\n",
    "        if(prediction[i]==testing_output[i]):\n",
    "            correct+=1\n",
    "        else:\n",
    "            wrong+=1\n",
    "        print(testing_image_paths[i] + ' = ' + classes[i])\n",
    "    print('correct = %d'%(correct))\n",
    "    print('wrong = %d'%(wrong))\n",
    "    print('average error = %d'%(error/10))\n",
    "    print(predicted_output)\n",
    "    print(testing_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
